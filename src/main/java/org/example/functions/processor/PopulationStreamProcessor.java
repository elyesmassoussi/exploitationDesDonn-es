package org.example.functions.processor;

import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.function.VoidFunction2;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Encoders;
import org.apache.spark.sql.SparkSession;
import org.apache.spark.streaming.Time;
import org.example.entities.beans.Population;
import org.example.functions.writer.PopulationWriter;
import org.json4s.Writer;

@Slf4j
@RequiredArgsConstructor
public class PopulationStreamProcessor implements VoidFunction2<JavaRDD<Population>, Time> {
    private final SparkSession sparkSession;
    private final String outputPathStr;

    @Override
    public void call(JavaRDD<Population> populationJavaRDD, Time time) throws Exception {
        long ts = System.currentTimeMillis();
        log.info("micro-batch time={} at stored in folder={}", time, ts);

        if(populationJavaRDD.isEmpty()){
            log.info("no data found!");
            return;
        }

        Dataset<Population> populationDataset = sparkSession.createDataset(
                populationJavaRDD.rdd(),
                Encoders.bean(Population.class)
        ).cache();


        populationDataset.printSchema();
        populationDataset.show(5, false);

        log.info("nb population = {}", populationDataset.count());


        PopulationWriter<Population> writer = new PopulationWriter<>(outputPathStr + "/time=" + ts);
        writer.accept(populationDataset);

        populationDataset.unpersist();
        log.info("done");
    }
}

