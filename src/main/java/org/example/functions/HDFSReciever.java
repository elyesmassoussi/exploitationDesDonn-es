package org.example.functions;

import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;

import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;

import org.apache.spark.api.java.function.Function;
import org.apache.spark.streaming.api.java.JavaDStream;
import org.apache.spark.streaming.api.java.JavaPairInputDStream;
import org.apache.spark.streaming.api.java.JavaStreamingContext;
import org.example.entities.beans.Population;
import org.example.entities.functions.TextToPopulation;

import java.util.function.Supplier;


@Slf4j
@RequiredArgsConstructor
public class HDFSReciever implements Supplier<JavaDStream<Population>> {

    private final String hdfsInputPathStr;
    private final JavaStreamingContext jsc;

    private final TextToPopulation textToPopulation = new TextToPopulation();
    private final Function<String, Population> mapper = textToPopulation::apply;

    Function<Path,Boolean> filter = hdfsPath -> {
        return !hdfsPath.getName().startsWith(".") &&
                !hdfsPath.getName().startsWith("_") &&
                !hdfsPath.getName().endsWith(".tmp");
    };


    @Override
    public JavaDStream<Population> get() {
        JavaPairInputDStream<LongWritable, Text> inputDStream =  jsc
                .fileStream(
                        hdfsInputPathStr,
                        LongWritable.class,
                        Text.class,
                        TextInputFormat.class,
                        filter,
                        true
                );
        JavaDStream<Population> javaDStream = inputDStream.map(t -> t._2().toString()).map(mapper);
        return javaDStream;
    }
}
