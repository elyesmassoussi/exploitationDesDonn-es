package org.example.functions.reader;


import lombok.SneakyThrows;
import org.apache.hadoop.fs.FileStatus;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import lombok.RequiredArgsConstructor;

import org.apache.spark.sql.SparkSession;

import java.util.Arrays;
import java.util.function.Supplier;

@RequiredArgsConstructor
public class Reader implements Supplier<Dataset<Row>> {

    private final String inputPath ;
    private final SparkSession sparksession;
    private final FileSystem hdfs;

    @SneakyThrows
    @Override
    public Dataset<Row> get() {

/*
        Dataset<Row> ds=sparksession.read().option("delimiter",";").option("header","true").csv(inputPath);
        ds.printSchema();
        ds.show(5, false);
        return ds ;


 */


        Path pathInput = new Path(inputPath);

        if(hdfs.exists(pathInput))
        {
            FileStatus[] listFiles = hdfs.listStatus(pathInput);

            String[] inputPaths =  Arrays.stream(listFiles)
                    .filter(t -> !t.isDirectory())
                    .map(f -> f.getPath().toString())
                    .toArray(String[]::new);

            Dataset<Row> inputDS = sparksession.read().option("delimiter",";")
                    .option("header","true").csv(inputPaths);

            inputDS.show(5,false);

            return inputDS;
        }

        return sparksession.emptyDataFrame();

    }
}



