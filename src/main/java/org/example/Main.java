package org.example;

import com.typesafe.config.Config;
import com.typesafe.config.ConfigFactory;
import lombok.extern.slf4j.Slf4j;

import org.apache.commons.io.IOUtils;
import org.apache.commons.lang3.math.NumberUtils;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.*;
import org.apache.spark.api.java.function.MapFunction;
import org.apache.spark.api.java.function.ReduceFunction;
import org.apache.spark.sql.*;
import org.example.entities.beans.Population;
import org.example.entities.beans.Stat;
import org.example.entities.functions.MapFunctionKey;
import org.example.entities.functions.MapPopulationToStat;
import org.example.entities.functions.PopulationMapper;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import scala.Tuple2;


import java.io.BufferedReader;
import java.io.IOException;
import java.io.InputStreamReader;
import java.time.LocalDateTime;
import java.time.format.DateTimeFormatter;
import java.util.Arrays;
import java.util.List;
import java.util.Map;
import java.util.Random;
import java.util.stream.Collectors;
import java.util.stream.Stream;
import java.util.stream.StreamSupport;

import static org.apache.spark.sql.functions.*;



@Slf4j
public class Main {
    private static Logger logger = LoggerFactory.getLogger(Main.class);
    private static DateTimeFormatter formatter = DateTimeFormatter.ISO_LOCAL_DATE_TIME;

    public static void main(String[] args) throws IOException{




        /*
        Config config = ConfigFactory.load("application.conf");
        String masterUrl = config.getString("master");
        String appName = config.getString("appname");
        SparkSession spark = SparkSession.builder().master(masterUrl).appName(appName).getOrCreate();
        String inputPath = config.getString("app.data.input");
        String outputPath = config.getString("app.data.output");
        Dataset<Row> ds=spark.read().option("delimiter",";").option("header","true").csv(inputPath);
        //Dataset<Row> statds=ds.groupBy("annee").agg(count("annee").as("nb"),sum("Population totale").as("expense"));
        ds.printSchema();
        ds.show(5, false);
        //System.out.println(statds);

        Dataset<Population> cleanDS = new PopulationMapper().apply(ds);
        cleanDS.printSchema();
        cleanDS.show(5, false);




        Config config = ConfigFactory.load("application.conf");
        String masterUrl = config.getString("master");
        String appName = config.getString("appname");

        SparkSession spark = SparkSession.builder().master(masterUrl).appName(appName).getOrCreate();
        Encoder<Population> encoder = Encoders.bean(Population.class);


        List<Population> data =
                List.of(Population.builder()
                                .annee("1992")
                                .popMunicipale("568932")
                                .popComptee("01004")
                                .popTotale("35789")
                                .build() ,
                        Population.builder()
                                .annee("1528")
                                .popMunicipale("15478")
                                .popComptee("122365")
                                .popTotale("15487")
                                .build(),
                        Population.builder()
                                .annee("1992")
                                .popMunicipale("4627")
                                .popComptee("5454")
                                .popTotale("212")
                                .build() ) ;

        Dataset<Population> dso = spark.createDataset(data,encoder);
        MapFunctionKey mfp = new MapFunctionKey() ;
        KeyValueGroupedDataset<String, Population> buckets = dso.groupByKey(mfp,
                Encoders.STRING());

        MapPopulationToStat mpts = new MapPopulationToStat() ;
        Encoder<Stat> encoderStat = Encoders.bean(Stat.class);
        Dataset<Tuple2<String, Stat>> result = buckets.mapValues(mpts, encoderStat).reduceGroups((ReduceFunction<Stat>) (s1, s2) -> new Stat(s1.getAnnee(), s1.getCount() + s2.getCount()));


        System.err.println("******************************************** hoyhoy ");
        System.out.println(result);

        log.info(result.toString());
        System.out.println("******************************************** hoyhoy ");

         */









        //cleanDS.write().mode(SaveMode.Overwrite).partitionBy("annee").json(outputPath);
        //cleanDS.write().mode(SaveMode.Overwrite).json(outputPath);














            Config config = ConfigFactory.load("application.conf");

            String appName = config.getString("appname");
            String appID = formatter.format(LocalDateTime.now()) + "_" + new Random().nextLong();

            FileSystem hdfs = FileSystem.get(new Configuration());

            logger.info("hdfsConf = <{}, {}, {}>", hdfs.getScheme(), appName, appID);

            /**DECLATION DES VARIABLES**/
            Path inputPath = new Path(config.getString("app.data.input"));
            Path outputPath = new Path(config.getString("app.data.output"));

            /**DATA PROCESSING**/
            logger.info("Listing files from inputPath={} ...", inputPath);
            RemoteIterator<LocatedFileStatus> fileIterator = hdfs.listFiles(inputPath, true);
            while(fileIterator.hasNext()){
                LocatedFileStatus locatedFileStatus = fileIterator.next();
                logger.info("fileInfo=<getName={}, getLen={}, getBlockSize={}, getPath={}>",
                        locatedFileStatus.getPath().getName(),
                        locatedFileStatus.getLen(),
                        locatedFileStatus.getBlockSize(),
                        locatedFileStatus.getPath().toString()
                );
                FSDataInputStream fsDataInputStream = hdfs.open(locatedFileStatus.getPath());
                BufferedReader reader = new BufferedReader(new InputStreamReader(fsDataInputStream));
                Stream<String> lines =
                        StreamSupport.stream(reader.lines().spliterator(), false)
                                .onClose(
                                        () -> {
                                            try{
                                                reader.close();
                                                fsDataInputStream.close();
                                            } catch (IOException ioe){
                                                logger.error("could not read {} due to...", locatedFileStatus.getPath().getName(), ioe);
                                                throw new RuntimeException(ioe);
                                            }
                                        }
                                )
                                .filter(l -> !l.startsWith("\"")); // remove header

                Map<String, Integer> wordCount = lines
                        .flatMap(l -> Arrays.stream(l.split(";")))
                        .filter(x -> !NumberUtils.isDigits(x))
                        .map(x -> new Tuple2<>(x, 1))
                        .collect(Collectors.toMap(Tuple2::_1, Tuple2::_2, Integer::sum));

                logger.info("wordCount");
                wordCount.forEach((k, v) -> logger.info("({} -> {})", k, v));

                String outlines = wordCount.entrySet().stream()
                        .map( e -> String.format("%s,%d", e.getKey(), e.getValue()))
                        .collect(Collectors.joining("\n"));
                FSDataOutputStream fsDataOutputStream = hdfs.create(outputPath.suffix(String.format("/wordcount/%s", locatedFileStatus.getPath().getName())));
                IOUtils.write(outlines, fsDataOutputStream, "UTF-8");
                fsDataOutputStream.close();
                fsDataInputStream.close();

            }

            /**DATA SAVING**/
            logger.info("Copying files from inputPath={} to outputPath={} ...", inputPath, outputPath);
            FileUtil.copy(
                    FileSystem.get(inputPath.toUri(), hdfs.getConf()),
                    inputPath,
                    FileSystem.get(outputPath.toUri(), hdfs.getConf()),
                    outputPath,
                    false, hdfs.getConf()
            );

            logger.info("Done");

        }


    }
