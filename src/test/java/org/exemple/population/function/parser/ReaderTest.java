package org.exemple.population.function.parser;


import com.jcraft.jsch.Logger;
import org.apache.hadoop.fs.*;
import org.apache.hadoop.fs.permission.FsPermission;
import org.apache.hadoop.util.Progressable;
import org.apache.spark.sql.SparkSession;
import org.example.functions.reader.Reader;
import org.junit.Test;



import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;
import org.junit.Test;

import java.io.FileNotFoundException;
import java.io.IOException;
import java.net.URI;

import static org.junit.Assert.assertEquals;
import static org.junit.Assert.assertNotNull;


public class ReaderTest {

    @Test
    public void readerTest() throws IOException {
        SparkSession spark = SparkSession.builder()
                .appName("MyDatasetReaderTest")
                .master("local[2]")
                .getOrCreate();
        String path = "target/test-classes/data.input/myFile.csv";
        FileSystem hdfs = FileSystem.get(spark.sparkContext().hadoopConfiguration());
        Reader reader = new Reader( path , spark, hdfs);
        Dataset<Row> dataset = reader.get();
        System.out.println("8888");
        assertNotNull(dataset);
        assertEquals(7, dataset.count());

    }

}
